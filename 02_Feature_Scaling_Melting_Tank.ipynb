{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Feature Scaling\n",
    "#### 작성: 고우주 | kubwa 쿱와"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/spaceship-preprocessing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[:500].drop('Transported', axis=1)\n",
    "y = df[:500]['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Standardization\n",
    "**z = (x - x_mean) /  std**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation: with the StandardScaler from sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the mean of the features, learned from train set\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the standard deviation deviation of the features,\n",
    "# learned from train set\n",
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Mean Normalization\n",
    "**x_scaled = (x - x_mean) / ( x_max - x_min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scalers - for mean normalisation\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_train.mean(axis=0)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the train set\n",
    "ranges = X_train.max(axis=0)-X_train.min(axis=0)\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform mean normalization:\n",
    "X_train_scaled = (X_train - means) / ranges\n",
    "X_test_scaled = (X_test - means) / ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mean = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "scaler_minmax = RobustScaler(with_centering=False,\n",
    "                             with_scaling=True,\n",
    "                             quantile_range=(0, 100))\n",
    "\n",
    "# fit the scalers to the train set, it will learn the parameters\n",
    "scaler_mean.fit(X_train)\n",
    "scaler_minmax.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler_minmax.transform(scaler_mean.transform(X_train))\n",
    "X_test_scaled = scaler_minmax.transform(scaler_mean.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-MinMaxScaling\n",
    "**X_scaled = (X - X.min / (X.max - X.min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler - for min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the maximum values of the features, learned from train set\n",
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the minimum values of the features, learned from train set\n",
    "scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler also stores the value range (max -  min)\n",
    "scaler.data_range_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-MaxAbsScaling\n",
    "**X_scaled = X / X.max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler - for MaxAbsScaling, with centering\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the maximum values of the features as learned from train set\n",
    "scaler.max_abs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the returned NumPy arrays to dataframes \n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Centering + MaxAbsScaling\n",
    "- 2개 Tranform을 사용 결합하여 분포 0을 중앙에 둔 다음 절대 최대값으로 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mean = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "# set up the MaxAbsScaler normally\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "\n",
    "# fit the scalers to the train set, it will learn the parameters\n",
    "scaler_mean.fit(X_train)\n",
    "scaler_maxabs.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler_maxabs.transform(scaler_mean.transform(X_train))\n",
    "X_test_scaled = scaler_maxabs.transform(scaler_mean.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-RobustScaling: quantiles and median\n",
    "**X_scaled = X - X_median / ( X.quantile(0.75) - X.quantile(0.25) )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the median values of the features as learned from train set\n",
    "scaler.center_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scaler stores the IQR values of the features as learned from train set\n",
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['Age'], ax=ax1)\n",
    "sns.kdeplot(X_train['Spa'], ax=ax1)\n",
    "sns.kdeplot(X_train['TotalPayment'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['Spa'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['TotalPayment'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-Normalizer: L1, L2\n",
    "**X_scaled_l1 = X / l1(X)**</br>\n",
    "**X_scaled_l2 = X / l2(X)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Norm 공식\n",
    "- `norm='l1'`으로 지정하면 절댓값인 L1 노름을 사용합니다. \n",
    "- 앞에서와 마찬가지로 샘플 별로 L1 노름을 계산한 다음 각 샘플을 나눕니다.\n",
    "\n",
    "$ \\lVert \\boldsymbol{x} \\rVert_1 = \\lvert x_1 \\rvert + \\lvert x_2 \\rvert + \\cdots + \\lvert x_n \\rvert $\n",
    "\n",
    "### L2 Norm 공식\n",
    "\n",
    "$ \\lVert \\boldsymbol{x} \\rVert_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} $\n",
    "\n",
    "- 먼저 샘플 별로 특성의 제곱을 더하기 위해 `axis=1`을 사용합니다. \n",
    "- 이 값의 제곱근을 구하면 L2 노름입니다. \n",
    "- 그 다음 각 샘플의 특성을 해당 L2 노름으로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l1') # for euclidean distance we change to norm='l2' \n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the norm for each observation (feature vector)\n",
    "np.round(np.linalg.norm(X_train, ord=1, axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled data\n",
    "np.round( np.linalg.norm(X_train_scaled, ord=1, axis=1), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
